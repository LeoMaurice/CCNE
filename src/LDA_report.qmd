---
title: "CCNE LDA"
author: "Léopold MAURICE, ENSAE"
date: "2023-07-11"
project: CCNE
supervision: Pr. Emmanuel Didier, CMH/ENS/EHESS
output-ext: html
format:
  html:
    toc: true
    number-sections: true
    colorlinks: true
    geometry:
      - top=10mm
      - left=20mm
      - right=20mm
      - bottom=10mm  
      - heightrounded
    highlight-style: github
    fontfamily: libertinus
    documentclass: report
    fig-width: 8
    fig-height: 8
  pdf:
    documentclass: report
    papersize: a4
    toc: true
    toc-depth: 4
    number-sections: true
    number-depth: 3
    colorlinks: true
    fig-width: 8
    fig-height: 8
    keep-tex: false
execute:
  eval: false
  message: false
  warning: false
  echo: false
  output: true
  error: true
editor: visual
---

citation_package: biblatex bibliography: references.bib

# Set up

```{r}
#| eval: false
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r}
#| eval: true

if (!require("pacman")) install.packages("pacman"); library(pacman)
pacman::p_load(tidyverse,
               lubridate,
               here,
               knitr,
               quanteda,
               quanteda.textstats,
               stm,
               topicmodels,
               LDAvis,
               ldatuning,
               readr,
               quanteda.textplots,
               ggprism,
               ggpmisc,
               RJSONIO,
               ggrepel
            )

# Some little helps from the internet
source("../src/helpers/lda_reports.R")

seed = 1968
set.seed(seed)
```

# Pre-processing

## tokenisation

```{r}
#| eval: true

BASE_CCNE <- read_delim("../data/exterior/BASE_CCNE_vf_1.tsv", 
    delim = "\t", escape_double = FALSE, 
    col_types = cols(Date = col_date(format = "%d/%m/%Y"), 
        Date2 = col_date(format = "%Y-%m-%d")), 
    trim_ws = TRUE)|>
  select(Num_avis,
         Date,
         Titre,
         txt_tot3,
         Annee)|>
  rename(num_avis=Num_avis,
         avis = txt_tot3)

BASE_CCNE$avis <- BASE_CCNE$avis|>
  str_replace_all("[']", " ") |>
  str_replace_all("[’]", " ") |>
  str_replace_all("[`]", " ")

# numbers and dates tokenisation

number_pattern <- "\\b\\d+\\b"

# Date pattern
date_pattern <- "\\b(?:\\d{4}-\\d{2}-\\d{2}|\\d{2}-\\d{2}-\\d{4}|\\d{4}-\\d{2}|\\d{2}-\\d{4}|\\d{2}-\\d{2})\\b"


# Replace numbers and dates with labels
BASE_CCNE$avis <- BASE_CCNE$avis |>
  gsub(pattern = date_pattern, replacement = "date_token")|>
  gsub(pattern = number_pattern, replacement = "number_token")

BASE_CCNE |> tibble::rowid_to_column("ID") -> BASE_CCNE
cp <- corpus(BASE_CCNE$avis, 
             docvars = BASE_CCNE |> select(
                                    avis, Titre, Annee, Date) |> as.data.frame(), 
             docnames = BASE_CCNE$ID)
# tokenisation
tk <- tokens(cp, remove_punct = TRUE, remove_numbers = TRUE)
```
## N-grams avant stopwords removed

```{r}
#| eval: true
#| output: true
#| fig-cap: "Examples de contexte suivant le mot 'don'"
#| label: tbl-contexte_don


kwic(tk, "don", window=2) |> 
  as.data.frame() |> 
  select("post") |> 
  table() |>
  sort(decreasing = TRUE) |>
  head(15)
```

```{r}
#| eval: true
#| output: true
#| fig-cap: "Examples de contexte suivant le mot 'personne'"
#| label: tbl-contexte_personne


kwic(tk, "personne", window=2) |> 
  as.data.frame() |> 
  select("post") |> 
  table() |>
  sort(decreasing = TRUE) |>
  head(15)
```
```{r}
#| eval: true
#| output: true
#| fig-cap: "Examples de contexte précédant le mot 'personne'"
#| label: tbl-contexte_personne


kwic(tk, "personne", window=4) |> 
  as.data.frame() |> 
  select("pre") |> 
  table() |>
  sort(decreasing = TRUE) |>
  head(15)
```

```{r}
#| eval: true
#| output: true
#| fig-cap: "Examples de contexte suivant le mot 'comité'"
#| label: tbl-contexte_comité


kwic(tk, "comité", window=4) |> 
  as.data.frame() |> 
  select("post") |> 
  table() |>
  sort(decreasing = TRUE) |>
  head(15)
```

```{r}
#| eval: true
#| output: true
#| fig-cap: "Examples de contexte suivant le mot 'code'"
#| label: tbl-contexte_code


kwic(tk, "code", window=4) |> 
  as.data.frame() |> 
  select("post") |> 
  table() |>
  sort(decreasing = TRUE) |>
  head(15)
```

```{r}
#| eval: true
#| output: true
#| fig-cap: "Examples de contexte suivant le mot 'non-commercialisation'"
#| label: tbl-contexte_non-commercialisation


kwic(tk, "non-commercialisation", window=3) |> 
  as.data.frame() |> 
  select("post") |> 
  table() |>
  sort(decreasing = TRUE) |>
  head(15)
```
```{r}
#| eval: true
#| output: true
#| fig-cap: "Examples de contexte suivant les mots'essai', 'expérience', 'expérimentation'"
#| label: tbl-contexte_exp


kwic(tk, c("essai","expérience","expérimentation", "essais","expériences","expérimentations"), window=3) |> 
  as.data.frame() |> 
  select("post") |> 
  table() |>
  sort(decreasing = TRUE) |>
  head(15)
```

RMQ : tous les dons correspondent il a un seul phénomène ? qui est celui du don comme acte gratuit. Perso je pense qu'il vaut mieux séparer car don de XXX réfère à un acte particulier. don seul réfère peut etre plus facilement alors au don en soit.

On pourrait essayer de faire la distinction de même entre les synonymes pour : - code - personne : humaine, agée, handicapée, handicapée mentale

Choix : V1 : don divisé en plusieurs catégories mais pas le terme personne. car je pense que quand on écrit personne handicapée. On désigne deux choses en un : d'une part la qualité de personne en soi, d'autre part le handicap.

Faut il inverser suppression des pluriels et choix des pattern ?

```{r}
#| eval: true

# equivalence between differents words (or multiples words)
important_expressions <- dictionary(list(
    vntr = c("variable number of tandem repeat", "variable number tandem repeat"),
    ccne = c("comité national consultatif d éthique",
             "comité national d éthique",
             "comité consultatif d éthique",
             "comité d éthique",
             "comité consultatif national d ethique",
             "comité national consultatif d ethique",
             "comité national d ethique",
             "comité consultatif d ethique",
             "comité d ethique",
             "comité consultatif national d ethique",
             "consultatif national d éthique",
             "Consultatif National d Éthique",
             "ce comité",
             "ccne",
             "ccné"),
    fin_de_vie = c("fin de vie"),
    #personne_humaine_potentielle = c("personne humaine potentielles",
    #                                 "personnes humaines potentielles"),
    #personne_humaine = c("personne humaine","personnes humaines"),
    #personne_agée = c("personne agée","personnes agées"),
    #personne_handicapee_mentale = c("personne handicapée mentale",
    #                                "personnes handicapées mentales")
    cellule_souche = c("cellule souche","cellules souches"),
    don_d_organe = c("don d organe","don d organes",
                     "dons d organe","dons d organes",
                     "don de moelle", "dons de moelle",
                     "don de moelles", "dons de moelles",
                     "don de tissu", "dons de tissu",
                     "don de tissus", "dons de tissus"),
    don_de_sang = c("don du sang","don de sang",
                    "dons du sang","dons de sang"),
    don_gamète = c("don d ovocyte", "don de gamète", "don d embryon",
                   "don d ovocytes", "don de gamètes", "don d embryons", 
                   "don de l embryon", "don de la gamète", "don de l ovocyte", 
                   "don de ces ovocytes", "don de ses ovocytes", "don de sperme", 
                   "don du sperme", "don de spermes",
                   "dons d ovocyte", "dons de gamète", "dons d embryon",
                   "dons d ovocytes", "dons de gamètes", "dons d embryons", 
                   "dons de l embryon", "dons de la gamète", "dons de l ovocyte", 
                   "dons de ces ovocytes", "dons de ses ovocytes", "dons de sperme", 
                   "dons du sperme", "dons de spermes"),
    don_cellule = c("don de cellule", "dons de cellule",
                    "don d'une cellule", "dons d'une cellule",
                    "don de cellules", "dons de cellules"),
    santé_publique = c("santé publique"),
    sécurité_sociale = c("sécurité sociale"),
    assurance_maladie = c("assurance_maladie"),
    ricoeur = c("ricoeur", "ricœur"),
    pma = c("amp","pma",
            "assistance médicale à la procréation",
            "assistances médicales à la procréation",
            "procréation médicalement assistée",
            "procréations médicalement assistées"),
    gpa = c("gpa",
            "gestation pour autrui"),
    vie_privée = c("vie privée",
                   "vies privées"),
    CRISPR-Cas9 = c("CRISPR-Cas9",
                    "CRISPR Cas9",
                    "CRISPR Cas",
                    "CRISPR-Cas",
                    "CRISPR",
                    "Cas9"),
    vih = c("vih","sida"),
    fiv = c("fertilisation in vitro",
            "fiv",
            "fivete",
            "ivf",
            "ICSI",
            "injection intracytoplasmique de spermatozoïde",
            "intra cytoplasmic sperm injection",
            "intracytoplasmic sperm injection"),
    expérience = c("essai","expérience","expérimentation")
    
))

# suppression pluriel, genre

tk <- tokens_compound(tk, pattern = important_expressions, case_insensitive = TRUE)

replacement_dict <- dictionary(list(
  embryon = c("embryon*"),
  parent = c("parent*"),
  enfant = c("enfant*"),
  éthique = c("éthiques"),
  médical = c("médical*"),
  droit = c("droits"),
  don = c("dons"),
  femme = c("femmes"),
  malade = c("malades"),
  patient = c("patients"),
  humain = c("humain*"), # risque de négliger les distinctions de genre ?
  personne = c("personnes","personne"),
  cellule = c("cellules"),
  test = c("tests"),
  génétique = c("génétiques"),
  problème = c("problèmes"),
  risque = c("risques"),
  question = c("questions"),
  résultat = c("résultats")
))

tk <- tokens_lookup(tk, replacement_dict, exclusive = FALSE)

# remove stop words, V1
toremove <- c(stopwords("fr"), stopwords("en"),
              c("number_token", "number_token-number_token"),
              c("être", "a", "plus", "peut", "comme", 
              "d’une", "cas", "d’un", 
              "si", "entre", "fait", "non", "doit", 
              "dont", "aussi", 
              "ainsi", "tout", "faire", "donc", 
              "très", "°", "peuvent", "chez", 
              "bien", "où", "toute",
              "autres", "elles", 
              "moins", "in", "après", 
              "encore", "notamment", "certains", 
              "alors", "pourrait", "mise",
              "part", "autre","tous", "possible",
              "exemple", "n’est", "avoir", "souvent","of",">","<","+","u","NA"),
              c("qu’il", "avant", 
              "c’est", "certaines", 
              "selon", "celle", 
              "doivent", "déjà", 
              "celui", "lors", "plusieurs", 
              "sous", "toujours", 
              "depuis", "toutes", "concernant", 
              "devrait", "seulement",
              "faut", "telle",
              "également", "cependant", "façon", "fois",
              "prendre", "point", "nécessaire", 
              "p", "partir", "donner",
              "dès", "ni", "nouvelles",
              "aujourd","hui","agit",
              "objet","place","projet",
              "deux","ment","e"
              )
)

# DFM format
dfm <- dfm(tk) |>
  dfm_lookup(important_expressions, exclusive = FALSE) |>
  dfm_remove(toremove) |>
  dfm_trim(min_termfreq = 10)

```

```{r}
textstat_frequency(dfm, n=100) |> select(feature) |> as.list()|> dput()
```

## Statistique descriptive

### WordCloud

```{r}
#| eval: true
#| output: true
#| fig-cap: Mots les plus fréquents dans le corpus. La taille est proportionnelle à la fréquence.
#| label: fig-wordcloud
palette <-ggprism::prism_color_pal("winter_bright")(9)
textplot_wordcloud(dfm, min_count = 750, random_order = FALSE, rotation = 0.25,
                   color = palette)
```

### Features similarity

```{r}
tstat_dist <- as.dist(textstat_dist(dfm))
clust <- hclust(tstat_dist)
plot(clust, xlab = "Distance", ylab = NULL)
```


### Evolution de mots choisies

```{r}
important_words <- c("personne", 
                     "nature",
                     "société",
                     "solidarité", 
                     "autonomie",
                     "dignité",
                     "liberté",
                     "consentement",
                     "non-commercialisation",
                     "avortement",
                     "pma",
                     "gpa",
                     "essai",
                     "vih",
                     "épidémie",
                     "cellule_souche",
                     "génétique", 
                     "ricoeur",
                     "belmont"
                     )
word_counts <- colSums(dfm[, important_words])

# Calculate total word counts in each document
total_words <- rowSums(dfm)

# Calculate frequencies
word_frequencies <- word_counts / total_words

# Create a data frame for plotting
plot_data <- data.frame(document = seq_along(documents), frequency = word_frequencies)

```


## On peut complexifier le pre processing

1.  Lemmatization ? pour éviter ce que j'ai déjà fait avec les pluriels ? : meh en français
2.  Supprimer les mots très rares, a minima les hapax (une occurrence dans le corpus);

# LDA

## Find number of topics

### Métriques

%### using topicmodels

```{r}
#| eval: true
#| output: true
#| fig-cap: Evolution des différentes métriques en fonction du nombre de topics
#| fig-subcap: Commenter métriques/résultats
#| label: fig-lda-metrics
tm_data <- quanteda::convert(dfm, to = "topicmodels")
tp_nb <- FindTopicsNumber(tm_data, topics = seq(5, 20), 
                          metrics = c("Griffiths2004", "CaoJuan2009", 
                                      "Arun2010", "Deveaud2014"),
                          method = "Gibbs")
FindTopicsNumber_plot(tp_nb)+
  theme_prism()+
  scale_colour_prism("winter_bright")
  
```

```{r ldatuning_plot_kable}
kable(tp_nb)
```

%### Avec STM

### exclusivité et cohérence

```{r}
#| eval: true

stm_data <- convert(dfm, to = "stm")

K <- seq(10, 20)
diag <- searchK(stm_data$documents, stm_data$vocab, 
                K, 
                verbose=FALSE)
```

```{r}
#| eval: true
#| output: true
#| fig-cap: Cohérence sémantique et excluvité en fonction du nombre de topics
#| label: fig-lda-exclu_cohsem
map(diag$results, unlist) |> 
  bind_cols() |> 
  ggplot(aes(exclus, semcoh, label = K)) +
    geom_point() +
    geom_label() +
  theme_prism()
```

## First model : classic LDA

```{r}
#| eval: true
num_topic = 11
```

### implémanté avec topicmodels

```{r}
#| eval: true
#| output: false

tm_lda <- LDA(tm_data, k = num_topic, method = "Gibbs", 
               control = list(seed = seed))
terms(tm_lda, 10) |> kable()
```

```{r}
#| eval: true
tm_lda_js <- topicmodels2LDAvis(tm_lda, reorder.topics = FALSE) # issu de ldareports
```

```{r}
#| eval: true
#| output: true
#| fig-cap: Top 10 mots par topic, estimation pour 14 topic avec topicmodels
#| label: tbl-lda-tm
report_topics_table(tm_lda)
```

```{r}
serVis(tm_lda_js)
```

### implémanté avec STM

```{r}
#| eval: false
#| output: true
#| fig-cap: Top 10 mots par topic, estimation pour 14 topic avec stm
#| label: tbl-lda-stm


stm_data <- convert(dfm, to = "stm")

stm_lda <- stm(documents = stm_data$documents, 
               vocab = stm_data$vocab,
               K=num_topic, 
               seed = seed, verbose = FALSE)

stm_lda$beta$logbeta[[1]] %>% 
  t() %>% 
  as_tibble() %>% 
  mutate(word = stm_lda$vocab, .before = V1) %>% 
  rename_with(~str_replace(.x, "V", "tp_"), starts_with("V")) %>% 
  mutate(across(starts_with("tp_"), ~exp(.x)))

```

```{r}
toLDAvis(stm_lda, stm_data$documents,reorder.topics = FALSE)
```

## Analyse

### Multidimensional Scaling

```{r}
#| eval: true
RJSONIO::fromJSON(tm_lda_js) -> tm_lda_ldavis
tm_lda_ldavis[["topic.order"]] -> ordre_ldavis
RJSONIO::fromJSON(tm_lda_js)[["mdsDat"]] |> 
  as.data.frame() |>
  rename(order = topics)|>
  mutate(topics = ordre_ldavis[order])-> tm_msd
```

```{r}
#| eval: false
ggpubr::ggscatter(tm_msd,x="x",y="y", size = "Freq", 
                  xlab = "PC1", ylab = "PC2",
                  alpha=0.6,color = "#00AFBB",
                  label = "topics",
                  label.rectangle = TRUE)+
  scale_x_continuous(limits = symmetric_limits) +
  scale_y_continuous(limits = symmetric_limits) +
  geom_quadrant_lines(linetype = "solid") +
  scale_size_area(max_size=12,limits=c(0,41), breaks = c(0,2,6,10,40))+
  theme_minimal()
```

```{r}
#| eval: true
#| output: true
#| fig-cap: Intertopic Distance Map (PCA, multidimensional scaling). Marker size is proportionnal to topic prevalence
#| label: fig-lda-PCA
ggplot(tm_msd, aes(x = x, y = y, label = topics)) +
  geom_point(aes(size = Freq), alpha = 0.6, color = "#00AFBB") +
  geom_label(
    box.padding = 0.3,
    point.padding = 0.1,
    size = 3,
    color = "black",
    nudge_x = -0.02,
    nudge_y = 0.00
  )+
  scale_x_continuous(limits = symmetric_limits) +
  scale_y_continuous(limits = symmetric_limits) +
  geom_quadrant_lines(linetype = "solid") +
  scale_size_area(max_size = 20, limits = c(0, 41), breaks = c(0, 2, 6, 10, 40)) +
  theme_minimal()
```

### Topics by documents

```{r}
#| eval: true

post <- posterior(tm_lda)
post$topics |> 
  as.data.frame() |>
  rename_all(~ paste0("topic ", .)) |>
  tibble::rowid_to_column("ID") |>
  left_join(BASE_CCNE, by="ID") -> tm_metadata
```

```{r}
#| eval: true
#| output: true
#| fig-cap: TOP3 documents where each topics is the more relevant
#| label: tbl-lda-top3doc

ordre_topics <- data.frame(topic = paste("topic", ordre_ldavis), ordre = 1:length(ordre_ldavis))

tm_metadata |>
  pivot_longer(cols = starts_with("topic "), names_to = "topic", values_to = "prevalence") |>
  left_join(ordre_topics, by = "topic")|> 
  group_by(topic) |>
  top_n(3, prevalence) |> 
  ungroup()|>
  arrange(ordre, desc(prevalence)) |>
  select("topic","num_avis","Titre","prevalence","Date","ordre") |>
  rename(Topic = topic, Prevalence = prevalence, "N° Avis"=num_avis) |>
  kable()
```

### Graph par années

```{r}
#| eval: true
#| output: true
#| fig-cap: Mean topic prevalence in the corpus by years
#| label: fig-lda-time

name_topic <- list(ordre = paste("topic",ordre_ldavis))
name_topic <- list(ordre = c(
                "éthique",
                "fin de vie",
                "système de soins",
                "données",
                "cellule souche",
                "science et nature",
                "test génétique",
                "procréation",
                "substance et cognition",
                "VIH",
                "chirurgie"))

tm_metadata|>
  select(Annee, starts_with("topic")) |>
  pivot_longer(-Annee, names_to = "topic") |>
  left_join(ordre_topics, by = "topic")|> 
  group_by(Annee, topic, ordre) |> 
  summarize(moyenne = mean(value)) |> 
  ggpubr::ggscatter(x="Annee", y = "moyenne", facet.by = "ordre", panel.labs = name_topic, xlab = "Année",ylab = "Mean Prevalence")+
  geom_line()+
  theme_prism()
```

%### with STM

```{r}

full_stm <- stm_data$meta |> 
  bind_cols(stm_lda$theta |> 
              as_tibble() |> 
              rename_with(~str_replace(.x, "V", "tp_"), starts_with("V")))
full_stm |> select(Annee, starts_with("tp")) |>
  pivot_longer(-Annee) |> 
  group_by(Annee, name) |> 
  summarize(m = mean(value)) |> 
  ggpubr::ggscatter(x="Annee", y = "m", facet.by = "name", xlab = "Année",ylab = "Mean Prevalence")+
  geom_line()+
  theme_prism()
```
