---
title: "ACL analysis"
author: "Léopold MAURICE, ENSAE"
date: "2024-02-29"
project: CCNE
supervision: Pr. Emmanuel Didier, CMH/ENS/EHESS
output-ext: html
format:
  html:
    toc: true
    number-sections: true
    colorlinks: true
    geometry:
      - top=10mm
      - left=20mm
      - right=20mm
      - bottom=10mm  
      - heightrounded
    highlight-style: github
    fontfamily: libertinus
    documentclass: report
    fig-width: 12
    fig-height: 9
  pdf:
    documentclass: report
    papersize: a4
    toc: true
    toc-depth: 4
    number-sections: true
    number-depth: 3
    colorlinks: true
    fig-width: 20
    fig-height: 15
    keep-tex: false
execute:
  eval: true
  message: false
  warning: false
  echo: false
  output: true
  error: true
editor: source
---

```{r}
require(pacman,quietly = F)

pacman::p_load(arrow,
               tidyverse,
               reshape2,
               rlang,
               tidytext,

               stats,
               rstatix,
               
               ggplot2,
               gridExtra,
               ggprism,
               ggpubr,
               ggcorrplot,
               ComplexHeatmap,
               circlize,
               patchwork)

source("helpers/df_to_matrix.R")
source("helpers/heatmap.R")
source("helpers/lexico_helper.R")
source("helpers/figures.R")

save_figures = T
cutpoints = c(0, 1e-04, 0.001, 0.01, 0.05, 0.1, 1)
symbols = c("****", "***", "**", "*", "•","ns")
significance = c("p < 0.0001", "p < 0.001", "p < 0.01", "p < 0.05", "p < 0.1", "p>0.1")
signif_caption_text <- paste("Significance levels:\n", paste(symbols, ":", significance, collapse = ", "))

```

```{r}
acl_proceedings <- read_delim("../data/ACL/acl_proceedings.csv", 
    delim = ";", escape_double = FALSE, trim_ws = TRUE)|>
  mutate(abstract = tolower(abstract),
         title = tolower(title))|>
  mutate(abstract = gsub("\\d", "", abstract))|>
  filter(!is.na(abstract))

keywords <- c(
  "neural networks"="neural network",
  "machine learning"="machine learning",
  "transformers"="transformer",
  "LLM"="llm|large language model",
  "topics model"="topic model|topics|topics model|LDA",
  "optimisation"="optimisation|optimization|optimize|optimise",
  "bert"="bert",
  "GPT"="gpt",
  "lexico"="lexico",
  "parsing"="parse|parsing",
  "chatbot"="chatbot|chat",
  "linguistic"="linguistic",
  "lemmatization"="lemmatisation|lemmatization",
  "syntax"="syntax",
  "name entity reco"="entity recognition|entities recognition",
  "translation"="translate|traduction|translation",
  "algo"="algorithm",
  "structure"="structure",
  "system" = "system",
  "task"="task",
  "tagging"="tagging",
  "grammar"="grammar",
  "morphology"="morphology|morphology",
  "classification"="classification",
  "annotation"="annotate|annotations"
)

for (key in names(keywords)){
  acl_proceedings <- acl_proceedings %>%
    mutate(!!key := str_count(abstract, pattern = keywords[key]))
}

```

```{r}
acl_proceedings %>%
  pivot_longer(cols = names(keywords), names_to = "Motsclés",values_to = "Fréquence")%>%
  mutate(sujet = Fréquence >0)%>%
  group_by(Motsclés,year)%>%
  summarise(Fréquence = 100*mean(sujet), rm.na= T)%>% 
  mutate(Motsclés = factor(Motsclés, levels = c(
    "linguistic",'grammar',"lexico","morphology","syntax",
    "structure","system","algo","parsing","translation","tagging",
    "chatbot","name entity reco","lemmatization","topics model",
    "optimisation","machine learning","annotation","classification","task",
    "neural networks","transformers","LLM","bert","GPT"
                                                )))%>%
  ggscatter(x = "year",y = "Fréquence",facet.by = "Motsclés", color = "Motsclés") +
  scale_y_log10()+
  labs(title = "Fréquence des documents avec certains mots clés au cours des années",
       x = "Année",
       y = "Fréquence des documents (échelle log des %)",
       color = "Mot clé") +
  theme_ready(text_size = 18)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  guides(color = FALSE)
ggsave("../output/acl/keywords_time.png", width = 28, height = 20, units = "cm")
```

```{r}
data(stop_words)
ngram_data <- acl_proceedings %>%
  unnest_tokens(output = word, input = abstract, token = "ngrams", n = 1)  # Change 'n' as needed for different ngram sizes

# Step 2: Remove stop words and possibly other irrelevant terms
ngram_data <- ngram_data %>%
  anti_join(stop_words)  # Assuming you have loaded stop words using `data(stop_words)`

# Step 3: Count the frequency of ngrams for each year
ngram_freq <- ngram_data %>%
  count(year, word) %>%
  arrange(n)%>%
  group_by(year)%>%
  top_n(5)

ngram_freq
```


