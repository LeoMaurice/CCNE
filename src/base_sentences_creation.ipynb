{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse du mot 'personne' dans les avis du CCNE   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code avec : \n",
    "tirage d'une base totalement aléatoire filtrée avec les mots d'intérêt pour exploration et rédaction de la guideline d'annotation\n",
    "export de cette base\n",
    "\n",
    "embedding des mots choisis\n",
    "\n",
    "tirage d'un set représentatif de phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leopo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "# The autoreload extension is already loaded. To reload it, use:\n",
    "#%reload_ext autoreload\n",
    "import sys\n",
    "sys.path.append('./helpers')  # Add the src directory to the Python path\n",
    "\n",
    "seed = 1968\n",
    "\n",
    "from database_creation import open_avis, join_metadata, corpus_to_sentences_with_context, filter_sentences_with_words, has_words\n",
    "from display import interactive_sentence_display, generate_sentences_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base mise au niveau de la phrase, et exemples pour guideline d'annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              corpus_to_sentences_with_context :\n",
      "              is  the number of sentences counted corresponding to the number of rows by 'num' ?\n",
      "              True\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Open Avis\n",
    "base_avis_ccne = open_avis(rescrap_texte=False)\n",
    "\n",
    "# Step 2: Join Metadata\n",
    "base_avis_ccne = join_metadata(base_avis_ccne)\n",
    "\n",
    "# Step 3: Transform to Sentences with Context\n",
    "base_sentences = corpus_to_sentences_with_context(base = base_avis_ccne)\n",
    "\n",
    "# Step 4: Filter on sentences with 'personne' in it\n",
    "base_filtered = filter_sentences_with_words(base_sentences,[\"personne\",\"personnes\",'humain','humains','humaine','humaines'])\n",
    "\n",
    "# Step 5: Sample 100 sentences\n",
    "samples = base_filtered.sample(100, random_state = seed)\n",
    "# le tirage original fut fait avec [\"personne\",\"personnes\",\"humain\",\"humains\",\"humaine\",\"humaines\"]\n",
    "# et surtout une fonction has_words qui regardait si la chaine \"personne\" était présente et donc \"personnel\" sortait positif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959d8a59d24745ada7986ee59f666a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=('Show Metadata', 'Hide Metadata'), value='Show Metadata')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd11f61f23e4332b5f9c5b72347743e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Previous', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d1d2d640e24dac8891740bcadf574a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4d7153646d4059b3356af279533b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: Display samples\n",
    "interactive_sentence_display(samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sentences_pdf(samples, \"../output/exploration/phrase_personne_humain.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Présence lexicale et export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présence lexicale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de phrases total dans les avis : 53591\n",
      "Le nombre de phrases qui contient ['personne', 'personnes'] est 5077\n",
      "Le nombre de phrases qui contient ['humain', 'humaine', 'humains', 'humaines'] est 2484\n",
      "Le nombre de phrases qui contient ['individu', 'individus'] est 658\n",
      "Le nombre de phrases qui contient ['société', 'sociétés', 'societé', 'societés', 'sociéte', 'sociétes', 'societe', 'societes'] est 1409\n",
      "Le nombre de phrases qui contient ['nature', 'natures'] est 651\n",
      "Le nombre de phrases qui contient ['environnement', 'environnements'] est 350\n"
     ]
    }
   ],
   "source": [
    "synonyms = {\n",
    "    'person': ['personne', 'personnes'],\n",
    "    'human': ['humain', 'humaine', 'humains', 'humaines'],\n",
    "    'individu': ['individu', 'individus'],\n",
    "    'societe': ['société', 'sociétés', 'societé', 'societés', 'sociéte', 'sociétes', 'societe', 'societes'],\n",
    "    'nature': ['nature','natures'],\n",
    "    'environment': ['environnement','environnements']\n",
    "    \n",
    "}\n",
    "\n",
    "print(f\"Nombre de phrases total dans les avis : {len(base_sentences)}\")\n",
    "\n",
    "for key in synonyms.keys():\n",
    "    has_specific_words = lambda x: has_words(x, words=synonyms[key])\n",
    "    base_sentences[key] = base_sentences['sentence'].apply(has_specific_words)\n",
    "    n = sum(base_sentences[key])\n",
    "    print(f\"Le nombre de phrases qui contient {synonyms[key]} est {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sentences.to_feather(\"../data/intermediate/big/base_sentences.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "\n",
    "# Load pre-trained CamemBERT model and tokenizer\n",
    "model = CamembertModel.from_pretrained('camembert-base')\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "# Define the list of words\n",
    "word_list = [\"personne\", \"personnes\", \"humain\", \"humains\", \"individu\", \"individus\"]\n",
    "\n",
    "# Tokenize the words\n",
    "tokenized_texts = [tokenizer.tokenize(word) for word in word_list]\n",
    "\n",
    "# Convert tokens to IDs\n",
    "indexed_tokens = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_texts]\n",
    "\n",
    "# Convert indexed tokens to PyTorch tensors\n",
    "tokens_tensor = [torch.tensor([indexed_tokens[i]]) for i in range(len(indexed_tokens))]\n",
    "\n",
    "# Get embeddings for each word\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for tensor in tokens_tensor:\n",
    "        outputs = model(tensor)\n",
    "        hidden_states = outputs[0]  # Last-layer hidden states\n",
    "        word_embedding = torch.mean(hidden_states, dim=1).numpy()  # Average the embeddings of all tokens\n",
    "        embeddings.append(word_embedding)\n",
    "\n",
    "# Compute pairwise cosine similarity/distance\n",
    "distance_matrix = np.zeros((len(word_list), len(word_list)))\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(len(embeddings)):\n",
    "        distance_matrix[i][j] = cosine_similarity(embeddings[i], embeddings[j])\n",
    "\n",
    "# Perform PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(np.concatenate(embeddings, axis=0))\n",
    "\n",
    "# Create DataFrame for Seaborn plotting\n",
    "df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "df['Word'] = word_list\n",
    "\n",
    "# Plot PCA result using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='PC1', y='PC2', hue='Word', s=100)\n",
    "for i in range(len(word_list)):\n",
    "    plt.text(df['PC1'][i], df['PC2'][i], df['Word'][i], fontsize=12)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('2D PCA Representation of BERT Embeddings')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Print distance matrix\n",
    "print(\"Distance Matrix:\")\n",
    "print(pd.DataFrame(distance_matrix, index=word_list, columns=word_list))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
